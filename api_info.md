I run lm studio local with the following api info:
model: TheBloke/Llama-2-7B-Chat-GGUF
file: llama-2-7b-chat.Q4_K_S.gguf
format: gguf
This model's API identifier: llama-2-7b-chat
link: http://127.0.0.1:1234/
support endpoints:

GET/v1/models

POST/v1/responses

POST/v1/chat/completions

POST/v1/completions

POST/v1/embeddings